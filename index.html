<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="Open-Vocabulary Affordance Localization for Task Oriented Grasping">
  <meta name="keywords" content="LLM, VLM, Open Vocabulary, Task Oriented Grasping">
  <meta property="og:title" content="OVAL-Grasp"/>
  <meta property="og:description" content="Open-Vocabulary Affordance Localization for Task Oriented Grasping"/>
  <meta property="og:url" content="https://ekjt.github.io/OVAL-Grasp/"/>
  <meta property="og:image" content="static/images/teaser.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="OVAL-Grasp"/>
  <meta name="twitter:description" content="Open-Vocabulary Affordance Localization for Task Oriented Grasping"/>
  <meta name="twitter:image" content="static/images/teaser.png"/>
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OVAL‑Grasp: Open‑Vocabulary Affordance Localization for Task Oriented Grasping</title>
  <link rel="icon" type="image/x-icon" href="static/images/robot.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              OVAL‑Grasp: Open‑Vocabulary Affordance Localization for Task Oriented Grasping
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ekjt.github.io/ekjtong/" target="_blank">Edmond Tong</a><sup>*1</sup>,
              </span>
              <span class="author-block">
                Advaith Balaji<sup>*1</sup>,
              </span>
              <span class="author-block">
                Anthony Opipari<sup>1</sup>,
              </span>
              <span class="author-block">
                Stanley Lewis<sup>1</sup>,
              </span>
              <span class="author-block">
                Zhen Zeng<sup>2</sup>,
              </span>
              <span class="author-block">
                Odest Chadwicke Jenkins<sup>1</sup>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>*</sup>Equal Contribution<br>
                University of Michigan<sup>1</sup> and J.P. Morgan AI Research<sup>2</sup><br>
                ISER 2025
              </span>
            </div>
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.11000" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/system_overview_v2.png" alt="System Overview">
        <h2 class="subtitle has-text-centered">
          The robot generates task-oriented grasps by using an LLM to identify grasp‑relevant object parts, a VLM to segment them, and a heatmap to filter grasp candidates to fulfill the given task.
        </h2>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL‑Grasp, a zero‑shot open‑vocabulary approach to task‑oriented, affordance‑based grasping that uses large‑language models (LLM) and vision‑language models (VLM) to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL‑Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task‑oriented grasping baselines on experiments with 20 household objects across three tasks each. OVAL‑Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real‑world experiments with the Fetch mobile manipulator. Additionally, OVAL‑Grasp finds correct object parts under partial occlusions, demonstrating an 80% part selection success rate in cluttered scenes, and we show the benefit of its modular design through ablation studies.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Summary Video</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/N0nZwmK__C4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Experiment Video</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/M2PrpD1ZlSI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Results</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/image.png" alt="Quantitative Results"/>
            <h2 class="subtitle has-text-centered">Quantitative Results</h2>
          </div>
          <div class="item">
            <img src="static/images/clutter_3.png" alt="Qualitative Results"/>
            <h2 class="subtitle has-text-centered">Qualitative Results</h2>
          </div>
          <div class="item">
            <img src="static/images/qual_results.png" alt="Qualitative Results"/>
            <h2 class="subtitle has-text-centered">Qualitative Results</h2>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{tong2024ovalprompt,
  title={OVAL‑Prompt: Open‑Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance‑Grounding},
  author={Edmond Tong and Anthony Opipari and Stanley Lewis and Zhen Zeng and Odest Chadwicke Jenkins},
  year={2024},
  eprint={2404.11000},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> (adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project).<br>
              Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY‑SA 4.0</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
